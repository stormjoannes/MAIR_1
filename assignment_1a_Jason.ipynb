{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73fbefbc-0b00-4849-a90f-5c187c6dcda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplicated data saved to deduplicated_data.txt\n",
      "Original Data Accuracy: 98.35%\n",
      "Original Data results saved to original_data_results.txt\n",
      "Deduplicated Data Accuracy: 90.80%\n",
      "Deduplicated Data results saved to deduplicated_data_results.txt\n",
      "Original Data Accuracy: 98.35%\n",
      "Original Data results saved to original_data_results.txt\n",
      "Deduplicated Data Accuracy: 90.80%\n",
      "Deduplicated Data results saved to deduplicated_data_results.txt\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "# Process files: read and classify line by line\n",
    "def process_file(filepath):\n",
    "    labeled_lines = []\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            # Ensure that line contains both label and sentence\n",
    "            if len(line.strip().split(\" \", 1)) == 2:\n",
    "                # Extract original label and sentence\n",
    "                original_label, sentence = line.split(\" \", 1)\n",
    "                original_label = original_label.strip()\n",
    "                sentence = sentence.strip()\n",
    "\n",
    "                # Store original label and sentence\n",
    "                labeled_lines.append((original_label, sentence))\n",
    "            else:\n",
    "                print(f\"Line skipped due to incorrect format: {line.strip()}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while processing file: {e}\")\n",
    "\n",
    "    return labeled_lines\n",
    "\n",
    "# Function to remove duplicates\n",
    "def remove_duplicates(labeled_lines):\n",
    "    seen_sentences = set()\n",
    "    deduplicated_lines = []\n",
    "\n",
    "    for label, sentence in labeled_lines:\n",
    "        if sentence not in seen_sentences:\n",
    "            deduplicated_lines.append((label, sentence))\n",
    "            seen_sentences.add(sentence)\n",
    "\n",
    "    return deduplicated_lines\n",
    "\n",
    "def save_deduplicated_data(input_file, output_file):\n",
    "    labeled_lines = process_file(input_file) \n",
    "    deduplicated_lines = remove_duplicates(labeled_lines) \n",
    "\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        for label, sentence in deduplicated_lines:\n",
    "            file.write(f\"{label} {sentence}\\n\")\n",
    "\n",
    "    print(f\"Deduplicated data saved to {output_file}\")\n",
    "\n",
    "save_deduplicated_data('dialog_acts.dat', 'deduplicated_data.txt')\n",
    "\n",
    "\n",
    "def plot_class_distribution_comparison(original_data, deduplicated_data):\n",
    "    # Convert data to DataFrames\n",
    "    df_original = pd.DataFrame(original_data, columns=['label', 'sentence'])\n",
    "    df_deduplicated = pd.DataFrame(deduplicated_data, columns=['label', 'sentence'])\n",
    "\n",
    "    # Calculate class distribution for each type\n",
    "    original_distribution = df_original['label'].value_counts().sort_index()\n",
    "    deduplicated_distribution = df_deduplicated['label'].value_counts().sort_index()\n",
    "\n",
    "    # Combine into a single DataFrame\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Original Data': original_distribution,\n",
    "        'Deduplicated Data': deduplicated_distribution\n",
    "    }).fillna(0)\n",
    "\n",
    "    # Plotting\n",
    "    ax = comparison_df.plot(kind='bar', figsize=(14, 8), width=0.8)\n",
    "    plt.title('Class Distribution Comparison')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "original_data = process_file('dialog_acts.dat')\n",
    "deduplicated_data = process_file('deduplicated_data.txt')  # Ensure this file exists and contains deduplicated data\n",
    "\n",
    "plot_class_distribution_comparison(original_data, deduplicated_data)\n",
    "\n",
    "# Function to train and evaluate the model, and save results to a file\n",
    "def train_and_evaluate(labeled_lines, description, output_file):\n",
    "    # Extract sentences and labels\n",
    "    sentences = [sentence for _, sentence in labeled_lines]\n",
    "    labels = [label for label, _ in labeled_lines]\n",
    "\n",
    "    # Convert text data into TF-IDF features\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.15, random_state=42)\n",
    "\n",
    "    # Train SVM classifier\n",
    "    svm_classifier = SVC(C=1.0, kernel='linear')  # Using linear kernel for simplicity\n",
    "    svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict labels for testing data\n",
    "    y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{description} Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Calculate confusion matrix and classification report\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=svm_classifier.classes_)\n",
    "    labels = svm_classifier.classes_  # Use the labels from the classifier\n",
    "    report = classification_report(y_test, y_pred, labels=labels, target_names=labels, output_dict=True, zero_division=0)\n",
    "\n",
    "    # Save the test results to a file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"Original Label\\tPredicted Label\\tSentence\\n\")\n",
    "        for original_label, predicted_label, sentence in zip(y_test, y_pred, [sentences[i] for i in X_test.indices]):\n",
    "            f.write(f\"{original_label}\\t{predicted_label}\\t{sentence}\\n\")\n",
    "\n",
    "    print(f\"{description} results saved to {output_file}\")\n",
    "\n",
    "    return cm, report, labels\n",
    "\n",
    "# Get results for original data\n",
    "cm_orig, report_orig, classes_orig = train_and_evaluate(original_data, \"Original Data\",\"original_data_results.txt\")\n",
    "\n",
    "# Get results for deduplicated data\n",
    "cm_dedup, report_dedup, classes_dedup = train_and_evaluate(deduplicated_data, \"Deduplicated Data\", \"deduplicated_data_results.txt\")\n",
    "\n",
    "# Ensure that classes are the same for both\n",
    "assert (classes_orig == classes_dedup).all(), \"Class labels should be the same for both datasets.\"\n",
    "\n",
    "\n",
    "def plot_confusion_matrices_and_reports(cm_orig, cm_dedup, report_orig, report_dedup, classes):\n",
    "    # Prepare classification reports\n",
    "    df_report_orig = pd.DataFrame(report_orig).T\n",
    "    df_report_dedup = pd.DataFrame(report_dedup).T\n",
    "\n",
    "    # Remove 'support' column from reports for a cleaner view\n",
    "    df_report_orig = df_report_orig.drop(columns='support')\n",
    "    df_report_dedup = df_report_dedup.drop(columns='support')\n",
    "\n",
    "    # Plot confusion matrices\n",
    "    plt.figure(figsize=(16, 12))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.heatmap(cm_orig, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.title('Confusion Matrix - Original Data')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.heatmap(cm_dedup, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.title('Confusion Matrix - Deduplicated Data')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    # Plot classification reports\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.heatmap(df_report_orig, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "    plt.title('Classification Report - Original Data')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    sns.heatmap(df_report_dedup, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "    plt.title('Classification Report - Deduplicated Data')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the results\n",
    "plot_confusion_matrices_and_reports(cm_orig, cm_dedup, report_orig, report_dedup, classes_orig)\n",
    "\n",
    "# Process files and create two datasets\n",
    "filepath = \"dialog_acts.dat\"  # File path containing original labels and sentences\n",
    "labeled_output = process_file(filepath)\n",
    "\n",
    "if labeled_output:\n",
    "    # Train and evaluate with original data and save results\n",
    "    train_and_evaluate(labeled_output, \"Original Data\", \"original_data_results.txt\")\n",
    "\n",
    "    # Remove duplicates\n",
    "    deduplicated_output = remove_duplicates(labeled_output)\n",
    "\n",
    "    # Train and evaluate with deduplicated data and save results\n",
    "    train_and_evaluate(deduplicated_output, \"Deduplicated Data\", \"deduplicated_data_results.txt\")\n",
    "else:\n",
    "    print(\"No data to process.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ed6ec1-5926-4815-a471-0c84f4030fb6",
   "metadata": {},
   "source": [
    "From the original data to the de-duplicated data, the accuracy decreased from 98% to 90.80%, especially in precision and recall.The categorisation results showed that\n",
    "1. inform and request categories still perform well in the de-duplicated data, but other categories (e.g., ack, deny, null) perform poorly.\n",
    "2. ack and deny categories have a recall of 0.00, probably because these categories have very few samples in the test set and the model cannot learn enough features.\n",
    "3. hello and repeat categories perform well in the de-duplicated data, especially in terms of precision and recall.\n",
    "There are several problems with the original and de-duplicated data, one is that the fifteen categories in the original data are highly skewed, and the second is that the categories in the de-duplicated data are not balanced, and the number of samples in certain categories is significantly reduced.\n",
    "Ultimately, the distribution of categories in the raw and de-emphasised data needs to be considered.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
